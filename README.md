### Languages
[![python](https://img.shields.io/badge/python-3.13-d6123c?color=white&labelColor=d6123c&logo=python&logoColor=white)](#)

---

### Frameworks
[![pandas](https://img.shields.io/badge/pandas-2.2.3-d6123c?logo=pandas&logoColor=white&color=white&labelColor=d6123c)](#)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.6.1-d6123c?logo=scikit-learn&logoColor=white&color=white&labelColor=d6123c)](#)
[![matplotlib](https://img.shields.io/badge/matplotlib-3.10.1-d6123c?color=white&labelColor=d6123c)](#)

---

## Table of Contents
- [Introduction](#introduction)
- [Screenshots](#screenshots)
  - [Most informative features by SHAP](#most-informative-features-by-shap)
  - [Final RMSLE](#final-rmsle)

# Introduction
This project is the final task for the Machine Learning course at [BigDataLab](https://www.bigdatalab.com.ua/).
The task was to train an ML model on a regression problem using the [House Prices dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview) and achieve an RMSLE lower than 0.15.

I used tree-based models/ensembles to reach this goal, such as **Decision Tree**, **Random Forest**, **XGBoost**, and **LGBM**.

# Screenshots
## Most informative features by SHAP
![Most informative features by SHAP](images/shap_most_informative_features.png)

## Final RMSLE 
![Result RMSLE](images/result.png)
