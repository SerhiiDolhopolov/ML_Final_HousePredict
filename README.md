<table align="center">
  <td colspan="5" align="center">
    <table>
      <tr>
       <td align="center">
        <img src="https://img.shields.io/badge/python-3.13-d6123c?color=white&labelColor=d6123c&logo=python&logoColor=white" alt="python">
        </td>
      </tr>
    </table>
    <table>
      <tr>
        <td align="center">
          <img src="https://img.shields.io/badge/pandas-2.2.3-d6123c?logo=pandas&logoColor=white&color=white&labelColor=d6123c" alt="pandas">
        </td>
        <td align="center">
          <img src="https://img.shields.io/badge/scikit--learn-1.6.1-d6123c?logo=scikit-learn&logoColor=white&color=white&labelColor=d6123c" alt="scikit-learn">
        </td>
        <td align="center">
          <img src="https://img.shields.io/badge/matplotlib-3.10.1-d6123c?color=white&labelColor=d6123c" alt="matplotlib">
        </td>
      </tr>
    </table>
  </td>
</tr>
</table>

## Table of Contents
- [Introduction](#introduction)
- [Screenshots](#screenshots)

# Introduction
This project is the final task for the Machine Learning course at [BigDataLab](https://www.bigdatalab.com.ua/).
The task was to train an ML model on a regression problem using the [House Prices dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview) and achieve an RMSLE lower than 0.15.

I used tree-based models/ensembles to reach this goal, such as **Decision Tree**, **Random Forest**, **XGBoost**, and **LGBM**.

# Screenshots
![Most informative features by SHAP](images/shap_most_influential_attributes.png)
Most informative features by SHAP

![Result RMSLE](images/result.png)
Final RMSLE